{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、反向传播#\n",
    "\n",
    "反向传播：训练模型参数，在所有参数上用梯度下降，使NN模型在训练数据上的损失函数最小。\n",
    "\n",
    "损失函数：计算得到的预测值y与已知答案Y_的差距。\n",
    "\n",
    "损失函数的计算方法有很多，均方误差MSE是比较常见的方法之一。\n",
    "\n",
    "均方误差MSE：求向前传播计算结果与已知答案只差的平方再求平均。\n",
    "\n",
    "用TensorFlow函数表示为：\n",
    "\n",
    "loss_MSE = tf.reduce_mean(tf.square(y_-y))\n",
    "\n",
    "反向传播训练方法：**以减小loss值为优化目标，有梯度下降、momentum优化器、adam优化器等优化方法。\n",
    "\n",
    "这三种优化方案用TensorFlow的函数可以表示为：\n",
    "\n",
    "1. train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)  使用随机梯度下降算法，使参数沿着梯度的反方向，即总损失减小的方向移动，实现更新参数。\n",
    "\n",
    "2. train_step = tf.train.MomentumOptimizer(learning_rate).minimize(loss)  在参数更新时，使用了超参数，\n",
    "\n",
    "3. train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)  利用自适应学习率的优化方法Adam算法和随机梯度下降算法不同，随机梯度下降保持单一的学习率更新所有参数，学习率在训练过程中并不会改变。而Adam算法通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立操作性。\n",
    "\n",
    "**学习率**：决定每次参数更新的幅度。\n",
    "优化器中都需要一个叫做学习率的参数，使用时，如果学习率选择过大会出现震动不收敛的情况，如果学习率选择过小，会出现收敛速度慢的情况。我们可以选择比较小的值填入，比如0.01、0.001。\n",
    "\n",
    "# 二、搭建神经网络的八股#\n",
    "\n",
    "0. 导入模块，生成模拟数据集\n",
    "import \n",
    "常量定义\n",
    "生成数据集\n",
    "\n",
    "1. 前向传播：定义输入、参数和输出\n",
    "x=……    y=……\n",
    "w1=……   w2=……\n",
    "a=……    y=……\n",
    "\n",
    "2. 反向传播：定义损失函数、反向传播方法\n",
    "loss=……\n",
    "train_setp=……\n",
    "\n",
    "3. 生成会话，训练STEPS轮\n",
    "with tf.session() as sess:\n",
    "    Init_op=tf.gobal_variables_initializer()\n",
    "    sess_run(init_op)\n",
    "    STEPS=3000\n",
    "    for i in range(SETPS):\n",
    "        start=\n",
    "        end=\n",
    "        sess.run(train_step, feed_dict:)\n",
    "        \n",
    "# 三、实际例子# \n",
    "\n",
    "随机产生32组生成出的零件的体积和重量，训练3000轮，每500轮输出一次损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [[0.83494319 0.11482951]\n",
      " [0.66899751 0.46594987]\n",
      " [0.60181666 0.58838408]\n",
      " [0.31836656 0.20502072]\n",
      " [0.87043944 0.02679395]\n",
      " [0.41539811 0.43938369]\n",
      " [0.68635684 0.24833404]\n",
      " [0.97315228 0.68541849]\n",
      " [0.03081617 0.89479913]\n",
      " [0.24665715 0.28584862]\n",
      " [0.31375667 0.47718349]\n",
      " [0.56689254 0.77079148]\n",
      " [0.7321604  0.35828963]\n",
      " [0.15724842 0.94294584]\n",
      " [0.34933722 0.84634483]\n",
      " [0.50304053 0.81299619]\n",
      " [0.23869886 0.9895604 ]\n",
      " [0.4636501  0.32531094]\n",
      " [0.36510487 0.97365522]\n",
      " [0.73350238 0.83833013]\n",
      " [0.61810158 0.12580353]\n",
      " [0.59274817 0.18779828]\n",
      " [0.87150299 0.34679501]\n",
      " [0.25883219 0.50002932]\n",
      " [0.75690948 0.83429824]\n",
      " [0.29316649 0.05646578]\n",
      " [0.10409134 0.88235166]\n",
      " [0.06727785 0.57784761]\n",
      " [0.38492705 0.48384792]\n",
      " [0.69234428 0.19687348]\n",
      " [0.42783492 0.73416985]\n",
      " [0.09696069 0.04883936]]\n",
      "Y: [[1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1]]\n",
      "w1: \n",
      "[[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]]\n",
      "w2: \n",
      "[[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "经过0步训练，所有数据上的损失为5.13118\n",
      "经过500步训练，所有数据上的损失为0.429111\n",
      "经过1000步训练，所有数据上的损失为0.409789\n",
      "经过1500步训练，所有数据上的损失为0.399923\n",
      "经过2000步训练，所有数据上的损失为0.394146\n",
      "经过2500步训练，所有数据上的损失为0.390597\n",
      "\n",
      "w1: [[-0.7000663   0.9136318   0.08953571]\n",
      " [-2.3402493  -0.14641267  0.58823055]]\n",
      "\n",
      "w2: [[-0.06024267]\n",
      " [ 0.91956186]\n",
      " [-0.0682071 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE = 8\n",
    "seed = 23455\n",
    "\n",
    "rng = np.random.RandomState(seed)\n",
    "X = rng.rand(32, 2)\n",
    "#生成正确答案：数据集的标签\n",
    "Y = [[int(x0+x1<1)] for (x0, x1) in X]\n",
    "print(\"X: \"+ str(X))\n",
    "print(\"Y: \"+ str(Y))\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,3], stddev = 1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3,1], stddev = 1, seed=1))\n",
    "\n",
    "a = tf.matmul(x,w1)\n",
    "y = tf.matmul(a,w2)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(y-y_))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    print(\"w1: \\n\"+str(sess.run(w1)))\n",
    "    print(\"w2: \\n\"+str(sess.run(w2)))\n",
    "    \n",
    "    #训练模型\n",
    "    STEPS= 3000\n",
    "    for i in range(STEPS):\n",
    "        start=(i*BATCH_SIZE)%32\n",
    "        end=start+BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
    "        if i%500 == 0:\n",
    "            total_loss = sess.run(loss, feed_dict={x:X, y_:Y})\n",
    "            print(\"经过%d步训练，所有数据上的损失为%g\"%(i, total_loss))\n",
    "    #输出训练后的参数取值\n",
    "    print(\"\\nw1: \"+ str(sess.run(w1)))\n",
    "    print(\"\\nw2: \"+ str(sess.run(w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
